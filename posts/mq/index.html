<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>消息队列 | Erica-Blog</title>
<meta name=keywords content="工作"><meta name=description content="消息队列 MQ是干嘛的？消息的中转站，能存很多消息。 应用场景： 应用解耦 ：消息队列减少了服务之间的耦合性，不同的服务可以通过消息队列进行通信，而"><meta name=author content="erica423"><link rel=canonical href=https://erica423.github.io/posts/mq/><link crossorigin=anonymous href=/assets/css/stylesheet.0fd2af49d7c8b1a279992ba82155f4ce3b1d94189758b3b0a99c8692cfdc0be9.css integrity rel="preload stylesheet" as=style><link rel=icon href=https://erica423.github.io/images/dls_icon.png><link rel=icon type=image/png sizes=16x16 href=https://erica423.github.io/images/dls_icon.png><link rel=icon type=image/png sizes=32x32 href=https://erica423.github.io/images/dls_icon.png><link rel=apple-touch-icon href=https://erica423.github.io/images/dls_icon.png><link rel=mask-icon href=https://erica423.github.io/images/dls_icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://erica423.github.io/posts/mq/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>(function(){""&&prompt("请输入文章密码")!==""&&(alert("密码错误！"),history.back())})()</script><link rel=stylesheet href=https://s1.hdslb.com/bfs/static/jinkela/long/font/regular.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Nunito:ital,wght@0,200..1000;1,200..1000&display=swap" rel=stylesheet><script src=https://unpkg.com/@waline/client@v2/dist/waline.js></script><link rel=stylesheet href=https://unpkg.com/@waline/client@v2/dist/waline.css><meta property="og:title" content="消息队列"><meta property="og:description" content="消息队列 MQ是干嘛的？消息的中转站，能存很多消息。 应用场景： 应用解耦 ：消息队列减少了服务之间的耦合性，不同的服务可以通过消息队列进行通信，而"><meta property="og:type" content="article"><meta property="og:url" content="https://erica423.github.io/posts/mq/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-18T13:42:19+08:00"><meta property="article:modified_time" content="2024-10-18T13:42:19+08:00"><meta property="og:site_name" content="erica-blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="消息队列"><meta name=twitter:description content="消息队列 MQ是干嘛的？消息的中转站，能存很多消息。 应用场景： 应用解耦 ：消息队列减少了服务之间的耦合性，不同的服务可以通过消息队列进行通信，而"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://erica423.github.io/posts/"},{"@type":"ListItem","position":2,"name":"消息队列","item":"https://erica423.github.io/posts/mq/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"消息队列","name":"消息队列","description":"消息队列 MQ是干嘛的？消息的中转站，能存很多消息。 应用场景： 应用解耦 ：消息队列减少了服务之间的耦合性，不同的服务可以通过消息队列进行通信，而","keywords":["工作"],"articleBody":"消息队列 MQ是干嘛的？消息的中转站，能存很多消息。 应用场景：\n应用解耦 ：消息队列减少了服务之间的耦合性，不同的服务可以通过消息队列进行通信，而不用关心彼此的实现细节。 异步处理 ：消息队列本身是异步的，它允许接收者在消息发送很长时间后再取回消息。 流量削锋 ：当上下游系统处理能力存在差距的时候，利用消息队列做一个通用的”载体”，在下游有能力处理的时候，再进行分发与处理。 日志处理 ：日志处理是指将消息队列用在日志处理中，比如 Kafka 的应用，解决大量日志传输的问题。 消息通讯 ：消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯，比如实现点对点消息队列，或者聊天室等。 消息广播 ：如果没有消息队列，每当一个新的业务方接入，我们都要接入一次新接口。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，是下游的事情，无疑极大地减少了开发和联调的工作量。 RocketMQ RocketMQ的相关组件和角色：\nproducer：生产者，消息的发送者。负责将消息发送给Broker producer group：生产者组，标记一类生产者 nameSrv：（看板）名字服务，一个路由注册中心。Broker会定时把自己的信息（IP等）上传给nameSrv，生产者和消费者就可以从nameSrv获取这些信息，才能顺利发送和接收消息 Broker：代理服务器，消息中转站，负责消息的存储、投递、查询 Broker cluster：代理服务器集群。可以是主主集群（集群内部Broker同级别同时对外提供服务），主从集群（主B对外提供服务，从B同步主B的消息作为备份防止主B宕机，不对外提供服务）。一个主可以有多个从 consumer：消费者，向Broker拉取自己想要消费的消息 consumer group：两个消费者组之间消费的消息互不干扰（比如一条消息可以被A组看也可以被B组看，想看多少各自随便） 对一个consumer group来说，消费的模式有：集群消费和广播消费\n集群模式：大家负载均衡地消费了这4条消息\n广播模式：组内每个消费组都收到了全部消息\nTopic：主题，消息的分类。这样对某一类消息感兴趣的消费者则可以订阅Topic-xxx，不会被无关的消息给冲烂了。\nTag：二级分类，更精细化区分消息\n思考：不同的消费组之间可以全量消费同Topic的消息，它们之间的消费进度又可以不一样，是Broker将信息复制了多份吗？\nmq两种基本实现模式 队列模式 生产者发送的消息被排成队列，然后消费者们竞争消费队列上的消息。按照队列的特性，消息被消费了等于出队，即从队列上被移除了，那么每条消息只会被一个消费者消费，因此消费者之间是竞争关系。\n缺点：一个消息可能有很多消费者都感兴趣，但是他们之间又不是竞争消费的关系，即这些消费者都想消费所有的消息。这时候普通的队列模式就不合适了。-\u003e 复制到多队列，加重存储负担，冗余\n发布-订阅模式 生产者发布消息，消费者订阅消息。订阅依据就是Topic主题。\n想要实现的功能：比如我往 Topic-LOL 这个主题发布消息，那么订阅了这个主题的消费者都能收到这个消息，我往 Topic-DOTA 这个主题发布消息，那么订阅了 DOTA 主题的消费者都能收到 DOTA 相关的消息。\n具体实现：引入消息位置（offset），类比数组下标。我们的述求是消息可以被多个消费者消费，那么只需维护每个消费者已经消费到的位置，每当消费者消费一条消息，消费位置就+1，然后消费者根据记录的消息位置去消费对应的数据即可。\n问题：同一个消费组内的消费者如何消费消息？让他们竞争同一个消费位置吗？那岂不是需要等上一个消费者消费完了，组内其他消费者才能消费下一条消息？这效率就很低了。\n-\u003e 引入：在 RocketMQ 中叫队列（这个跟数据结构上的队列在概念上要区分下），在Kafka中叫分区。\n可以看到，发往一个 Topic 的消息，实际上不是在一个队列里，而是分布在多个队列中。这样属于一个消费组的消费者们可以专门负责主题里面的一个队列。\n消费点位的记录维度就变成了 Topic-消费组-队列，比如现在一共有两个主题，分别是 Topic-LOL、Topic-DOTA，每个主题都有两个队列（分区）。\n如果c组有加了一人c，对应Topic也可以加一个队列；另一个c组人数不变，可以让其中一人多消费一个队列即可。\n处理消息堆积：加队列同时加消费者（面试常问）\n至此，我们就清晰了企业级消息队列实现的发布-订阅模式的核心原理：即 Topic 下分队列（分区），然后维护每个消费组在每个 Topic 下每个队列的消息位置，以消息位置（offset）来控制消息消费的进度。\n问题：一个Topic里有多个队列，生产者如何确定往哪个队列发消息？\n轮询：比如生产者-A，要往 Topic-LOL 发送消息，那么第一条发给队列-1，第二条发给队列-2，第三条发给队列-3，第四条发给队列-1，第五条发给队列-2。。。。如此往复即可。这样每个队列的消息量平均，对应消费者的工作量也平均。 指定发往某个队列：比如有关匹配的消息都发往队列-1，有关大乱斗的消息都发往队列-2，有关云顶之弈的消息都发给队列-3。 RabbitMQ 的底层就是队列模式，而 RocketMQ 和 Kafka 两者的实现都是发布-订阅模式。\nBroker消息存储 消息存储非常重要，mq一个关键功能就是削峰填谷。比如在大流量场景下（双十一），一下子涌入大量用户，造成请求高峰，使系统压力剧增。\n解决：把相关请求先发送到mq，然后直接返回success给用户。如：“业务受理成功，由于当前访问较多，详情约在xx-xx分钟后可查询“这样的话术。然后让系统平缓、匀速地拿取mq里的消息再进行业务处理。\n而这个过程的重点就是保证排队消息存储的可靠性。\nso，，Broker里的消息应该存到哪里呢？\n存数据库（MySQL）？zszz，性能不行； 那缓存Redis？性能是ok，but不靠谱； 对于一个中间件，mq本身轻量级也很关键，中间件依赖别的组件，就是强行使用者维护更多的组件，会变得更复杂。\n所以存本地硬盘。其实mysql保存和redis持久化的数据也都是在本地硬盘，所以直接跳过中介，把消息直接写入本地硬盘的文件上。现在硬盘都是RAID，即多块硬盘组成逻辑上的一块，也就是有备份。 那么mq中的消息要以什么格式存储到文件里呢？\ncommitlog ：类似记事本一样，一行一行的写\n每条消息的大小不固定；\n不同topic的消息都存在一个文件里（why都写进一个文件？涉及计组的底层机械硬盘结构）\n计组知识点\n寻道时间 旋转时间 在物理上，如果存储在硬盘上的数据在同一个磁道且相邻的扇区，那么根据硬盘的机械运行轨迹，读/写的顺序就非常快（省略寻道时间，顺应旋转的方向），甚至可以媲美内存的读写速度！顺序读 顺序写\n顺序写就是RocketMQ为什么性能好的原因（面试问）其他应用：mysql的redo log\nso，所有的消息不论是哪个 topic 都写入同一个文件，并且都是顺序追加写入，那么对应到硬盘上就是顺序写！（基本上一个文件的内容，如果空间足够，那么都是连续的）这样就大大提高了消息队列写消息的性能。\n如果将不同的 topic 存入不同的文件，我们无法保证这些文件在物理上的位置是连续的。(随机写)\n总结： RocketMQ 选择将消息写入到文件中，依赖机械硬盘（当然也可以是ssd）来保证消息存储的可靠性，并且根据机械硬盘的特性，把不同 topic 的消息都追加写入到一个叫 commitlog 的文件中，这样的顺序写性能很好。\n现在我们知道消息都存储在一个叫commitlog的文件里，那么队列呢？队列和commitlog是什么关系？消费者如何快速获取commitlog里的消息？\nConsumeQueue 当消息被写入commitlog时表示生产者成功发送了消息，消息也落入硬盘被持久化了。此时可以启动一个定时任务 (约1ms/次，几乎实时)，将新写入commitlog的消息转发给consumeQueue(就是Topic队列)\nconsumequeue有必要存全量的消息吗？完全copy一份commitlog也太消耗内存了。\n所以直接去cl里找。由于消息直接存入cl，且是追加写入，里面存储了连续的消息。引入偏移量作索引。\nconsumerQueue 实际存储的不是消息本身，而是存储消息在 commitlog 文件中的起始偏移量和本条消息的长度还有tagHash。\n时间换空间 也需要落盘存储\nconsumerOffset 和 commitlog offset：\nconsumeroffset 消息（逻辑）位置：在一个队列被多个消费者消费时，只需维护每个消费者已经消费到的位置。每当消费一条消息，offset（消费位置）+1，然后消费者根据记录的offset去队列里查找消费对应的消息。\ncommitlog里的offset：消息所在commitlog文件中的偏移位置。通过这个offset找到消息的起始位置再+size获取完整消息\n至此，消费者消费一条消息的流程：\n通过consumerOffset找到consumerQueue里的内容，这个内容存储了消息所在commitlog中的offset和size，然后通过这两个数据去commitlog里获取完整的消息内容。\n消息索引 IndexFile 通过输入索引快速定位到一条消息。\nHashMap，但文件不提供HashMap的功能\nIndexFile结构：\nhead：大小固定，存放一些元信息（当前文件中消息的最小和最大存储时间、当前文件中消息在commitlog的最小和最大偏移量、已用hash槽个数、已用index个数）\n每个槽大小固定4字节\n知道是第几个槽后，槽里存的是index item下标。每个index item大小固定20字节。一个indexFile文件有2000w个ii，存储了消息再commitlog中的物理offset。\n假如发生了hash冲突怎么办？\n更新槽的值为最新的index item下标，1。并且记录上一个index item 的下标，prevIndex = 0。同时index item里还要存储keyhash来判断跟当前计算的hash是否一致，不一致说明冲突，需要通过prevIndex值去得到上一个item的下标再去查找。 index item里还存了消息的时间，这个时间还是个差值：这条消息在commitlog写入的时间（IndexFile第一条消息的写入时间）\n消息发送 消息类型：\n普通消息\n同步消息：producer发送一条消息给Broker，需要等待Broker返回响应才会继续发送后续消息\nproducer.send(msg)\n要是没收到reaction，生产者会进行重试。默认重试3次，3次后还是失败就抛异常。此时需要我们捕获这个异常，进行日志记录或其他兜底操作。\n（类似TCP的单词握手，一次request过去，一个ack回来）\n但酱紫重试的缺点是消息重复发送\n这里要做好消息的防重，或幂等（？）\n问：如何保证消息一定发送成功？\n答：Broker接收成功后，返回ack给生产者，没接收到ack则不认为消息发送成功，进行重试。\n异步消息\n不需要阻塞等待上一条的ack可以紧接着发送后续消息\n假如前面的消息发送失败了，会有另外的线程来处理Broker的响应。成功响应就执行onSuccess逻辑，失败则执行onException\n1 2 3 4 5 6 7 8 9 10 11 producer.send(msg, new SendCallback(){ public void onSuccess(SendResult sendResult) { //do sth } public void onException(Throwable e) { //do sth // 失败信息记录 } }) 异步消息也能设置重试，问题和同步的一样（重复）。适合对响应时间敏感度的场景。\n单向消息\n生产者只管发消息，不关心Broker收没收到，也不用等ack\n适合对消息可靠性要求不高的场景: 大量日志收集（不在乎丢几条）\n顺序消息\n按顺序发布消息，按发布顺序消费消息\n场景：1. 创建订单 2. 支付 3. 发货 4. 完结订单\n由于一个Topic多队列的实现，普通消息没法保证先发送的一定先被消费。\n解决方案：把这几个消息都发一个队列里！把这几类消息都发进一个队列，这种叫全局顺序消息\n如果把同一笔订单的创建支付发货收货发往一个队列，不同订单发往不同队列，叫分区顺序消息. 能提高并发度和消费速度。\n分区依赖一个sharding key，比如订单场景中的订单号可以作为sharding key。\n1 2 queueIndex := orderId % len(queue) producer.send(msg, queueIndex) 默认普通消息是轮询选择队列。（上一个发队列1，下一个发队列2，。。。）\n延迟消息\n生产者发送了消息，希望延迟一段时间再被消费。\n实现：\n延迟消息一开始不放在正常的Topic中，有一个专门的Topic叫SCHEDULE_TOPIC_XXXX，放置所有的延迟消息。等到时间了再把该消息发回本身的Topic队列。这样就不会妨碍消费其他消息。\nRocketMQ中的延迟时间只能设定投递等级，不同等级固定对应一个时间。\n批量消息\n一次发包发送多条消息，适用对吞度量敏感的场景。\n100条打包成1条发，只调用一次接口，只等待一次reaction。吞度效率变高了，但一条消息有错就要一批重来。\n事务消息\nNameSrv 命名服务 NameSrv和Broker 在 Broker 启动之前，NameSrv 需要先启动。Broker 会定时的把自己关于 Topic 的信息上报至 NameSrv，NameSrv 会维护记录这些信息，并且将无用的 Broker 剔除。\nNameSrv和Producer 对于 Producer 来说，启动后它需要找到一台 NameSrv 建立长连接。Producer 需要从 NameSrv 或者有关 Topic 的路由信息，然后跟对应的 Broker 建立长连接，后面直接将消息发送给对应的 Topic。\n注意：Producer 不会缓存路由信息到本地文件中，只会放在内存里。所以如果 NameSrv 挂了，由于 Produer 是直接跟 Broker 建立长连接直接发送消息的，所以不影响现有的消息发送。\n但是如果这时候你将 Producer 重启了，那么在 NameSrv 挂了的情况下，消息就无法正常发送了，因为路由信息都丢了，也无法从 NameSrv 获取。不要随意重启Producer\nNameSrv和Consumer Consumer和Producer一样，启动后先和NameSrv建立长连接，获取topic和broker的映射。然后和有关broker建立长连接。发消息和消费消息的过程都不会经过中间商NameSrv，NameSrv 的作用就是“看板”，来记录维护路由信息，给 Consumer 和 Producer 指路。\nNameSrv和NameSrv 为了避免单点NameSrv挂了导致新P和C成瞎子，在生产中需要部署多台NameSrv形成集群。RocketMQ 实现的 NameSrv 非常轻量级，集群内的 NameSrv 是互相独立的存在，它们之间不会进行任何的信息交互。\nBroker需要给集群内的每一台NameSrv都上报路由信息。 对 Producer 和 Consumer 来说，因为多台 NameSrv 的数据都是一样的，因此它们只需要随机选择集群内的一台 NameSrv 进行长连接即可获取全量的路由信息。 总结：\nNameSrv 这个角色就是动态路由中心，会维护存活的 Broker 信息，记录 Topic 的路由关系，使得 Producer 和 Consumer 可以正确的找到对应的 Broker 。\nRocketMQ 简单的自己设计了一个轻量级路由中心而不是依赖于其他三方组件，这样对整个消息队列中间件而言更轻量和自主。\nkafka中对应NameSrv的角色是zookeeper。zk 集群中多台机器会进行数据同步，就会产生数据一致性问题，NameSrv 则直接每台都存储全量的数据，且之间不进行任何信息交互，完美地避开数据不一致的问题。\nkafka 在新版本中已经移除了 zk 的强依赖（强一致性在性能方面开销会比较大）。\n新问题：消息存储在Broker，是消费者去Broker拉取消息pull还是B主动给消费者推push呢？\n答：看需求\n消息vs快递 ：上门送货 = push， 自取 = pull\n推消息 Broker收到对应消息，立马推送给消费者\n优点：实时性超高，对消费者实现也简单；\n缺点：消费者在某个时段超忙，没空处理新消息；Broker疯狂push，可能直接把消费者干爆；这就违背了消息队列削峰填谷的初衷。所以Broker在push消息前可以先打探消费者的状态再推，但消费者太多的情况也不合适。\n使用场景：消费者不多、消息量不大、及时性要求高\n拉消息 优点：Broker轻松，只需要存储好生产者发来的消息，等消费者来拉就好了，想拉多少拉多少，不用维护其他关系；\n缺点：消息的消费可能不及时\n如果把定时时间调短一点，消费不及时可以改善，但又造成了消息忙请求。就是说几个小时都没消息过来，消费者还隔1s来要一次。\n目前RocketMQ和kafka用的都是拉模式 pull，但是长轮询：\n消费者发送拉取请求到Broker时，如果此时有消息，则Broker直接响应返回消息；如果没消息就hold这个请求，比如等待15s，15s内要是有消息过来了就立马响应返回。\n这样既避免了忙请求，也提升了消息的及时性ovo\nRMQ有pullConsumer和pushConsumer，但底层实现都是基于长连接的长轮询去拉取消息。\n消费者启动 怎么获取nameSrv？在开始时注入nameSrv地址\n接下来分赃：\n新来一个消费者分俩给它，减轻老消费者的压力同时提高消费效率。\n分赃的契机：新消费者启动并连上Broker\n分赃过程：由消费者遵循一定规则主动分配 （专名：==重平衡==，也是==客户端的负载均衡==）\n消费者5看似没活干了，会这么摸鱼吗？\n问：消息堆积时，增加消费者有用吗？\n答：我不好说。如果这个消费者组的消费者数量比Topic的队列数量小，此时可以加消费者来缓解消息堆积；如果消费者数量比队列还多，就没用力。\n不光在消费者启动时会触发重平衡，每个消费者会有个定时任务，每20s重平衡一下子，就负载均衡了，防止消费任务不均衡。 如果有消费者下线了，由于和Broker有连接，下线了Broker也知道，会通知所有消费者重平衡一下。 实现了动态负载均衡：业务高峰时增加消费者，业务低谷时减少消费者。\n怎么拉消息 最简单的想法是一个队列对应一个线程负责去pull，但上下文切换过于频繁。线程池？也用不上\nRocketMQ只用了一个线程（PullMessageService）来执行pull操作，所有的pull动作都会封装成pullRequest，扔到pullRequestQueue一个阻塞队列中。\npullRequest获取的结果会先缓存到ProcessQueue中，同时再构建一个消费任务consumerRequest 给ConsumeMessageService这个线程池来消费消息。\nProcessQueue: 起到暂缓消息的作用，用来流控。如果暂缓的消息很多，说明消息处理慢，消息在消费者这里有堆积，这时候就需要限制拉取速度。也就是不能一直构建pullRequest，要等一等（流控）\n消息点位 offset：之前提到的每个消费组需要维护Topic下的每个队列被消费到的点位，这个点位是怎么保存的呢？在什么时候提交呢？\n答：在不同模式下，保存方式不同\n广播模式：\n​\t广播模式下存储在消费者本地磁盘上，因为消息广播给每个消费者，不需要统一一个地方来管，每个消费者自己维护就行了。\n集群模式：\n​\t不能本地维护了，因为同个消费组内的消费者是互帮互助的关系。在新消费者顶替旧消费者时需要知晓前一个人的消费进度。所以集群模式下offset存储在Broker里，这样顶上的消费者可以从Broker获取offset。\n那么，消费者具体在什么时候将自己的消费位置告知Broker更新呢？\n答：拉取信息的时候顺带把消费点位提交给Broker。\n问题又来了：假设消费完了，还没来得及拉消息，消费者就挂了，此时存在Broker里的点位不就不准了吗？\n答：确实，按照这个设计此时如果发生了重平衡，就会导致消息重复消费。这个设计没法保证消息只会被消费一次。因此我们只能做好消费者的幂等，使得消费多次消息和消费一次达到的效果一样。\n扩展：常用的业务幂等性保证方法\n一文讲透消息队列RocketMQ实现消费幂等-腾讯云开发者社区-腾讯云 (tencent.com)\n只靠mq无法实现业务幂等性，要结合业务场景 利用数据库的唯一约束实现幂等：如将订单表中的订单编号设置为唯一索引，创建订单时，根据订单编号就可以保证幂等； 去重表：和上一个本质相同，首先在去重表上建唯一索引，其次操作时把业务表和去重表放在同个本地事务中，如果出现重现重复消费，数据库会抛唯一约束异常，操作就会回滚 利用redis的原子性：每次操作都直接set到redis里，然后将redis数据定时同步到数据库中； 多版本（乐观锁）控制：适用于更新场景。给业务数据增加一个版本号属性，每次更新数据前，比较当前数据的版本号是否和消息中的版本一致，如果不一致则拒绝更新数据，更新数据的同时将版本号+1 Token机制：生产者发送每条数据的时候，增加一个全局唯一的id，这个id通常是业务的唯一标识，比如订单编号。在消费端消费时，则验证该id是否被消费过，如果还没消费过，则进行业务处理。处理结束后，在把该id存入redis，同时设置状态为已消费。如果已经消费过了，则不进行处理。 状态机机制：多用于更新且业务场景存在多种状态流转的场景 怎么消费消息 前面提到 pull 到消息后会构建ConsumeRequest 提交到线程池 ConsumeMessageService。\n线程池有两个实现：\n并发消费（常用） 顺序消费 假设线程池消息消费失败了咋办？\nRMQ的实现是将消息消费失败的消息返回发送给Broker，这个消息会被发送给一个特定的重试Topic： “%RETRY%+consumerGroup”\n这样就失败消息就不会阻塞原来Topic上的其他消息，而是在重试队列上等待后续的重试消费；如果多次重试还是失败（默认16次），就会把它打入死信队列，需要人工介入处理。\n总结 消息在mq上生产、消费的关键链路流程:\nnameSrv启动，待命； Broker启动，把自己的IP、端口、Topic等上传给nameSrv，等待Producer发送消息，等待Consumer消费消息； Producer启动，连接nameSrv并获取Broker信息，跟对应Broker建立连接，连上之后发消息给Broker； Broker将消息存储到commitlog文件中，并分发到consumeQueue，等待consumer来拉取消息消费； Consumer启动，连接nameSrv并获取Broker信息，跟对应Broker建立连接，连上后发送pullRequest给Broker； Broker根据对应的Topic、队列ID和消息点位，找到consumeQueue中的消息，再解析找到对应commitlog得到消息内容，返回给Consumer; Consumer消费消息，随后上报自己的消费进度给Broker。 【持续更新…】\n","wordCount":"8858","inLanguage":"en","datePublished":"2024-10-18T13:42:19+08:00","dateModified":"2024-10-18T13:42:19+08:00","author":{"@type":"Person","name":"erica423"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://erica423.github.io/posts/mq/"},"publisher":{"@type":"Organization","name":"Erica-Blog","logo":{"@type":"ImageObject","url":"https://erica423.github.io/images/dls_icon.png"}}}</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://erica423.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://erica423.github.io/images/dls_icon.png alt aria-label=logo height=25>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://erica423.github.io/archives/ title=🎮动态><span>🎮动态</span></a></li><li><a href=https://erica423.github.io/tags/ title=🎲标签><span>🎲标签</span></a></li><li><a href=https://erica423.github.io/search/ title="🎯搜索 (Alt + /)" accesskey=/><span>🎯搜索</span></a></li><li><a href=https://github.com/Erica423/Erica423.github.io/ title=Github><span>Github</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://erica423.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://erica423.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">消息队列</h1><div class=post-meta><span title='2024-10-18 13:42:19 +0800 +0800'>October 18, 2024</span>&nbsp;·&nbsp;erica423&nbsp;|&nbsp;<a href=https://github.com/Erica423/Erica423.github.io rel="noopener noreferrer" target=_blank>主页</a><div class=meta-item>&nbsp·&nbsp
	      <span id=busuanzi_container_page_pv>本文阅读量<span id=busuanzi_value_page_pv></span>次</span></div></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#消息队列>消息队列</a></li><li><a href=#rocketmq>RocketMQ</a><ul><li><a href=#mq两种基本实现模式>mq两种基本实现模式</a></li><li><a href=#broker消息存储>Broker消息存储</a></li><li><a href=#consumequeue>ConsumeQueue</a></li><li><a href=#消息发送>消息发送</a></li><li><a href=#namesrv-命名服务>NameSrv 命名服务</a></li><li><a href=#总结>总结</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=消息队列>消息队列<a hidden class=anchor aria-hidden=true href=#消息队列>#</a></h2><ul><li>MQ是干嘛的？消息的中转站，能存很多消息。</li></ul><p>应用场景：</p><ul><li><strong>应用解耦</strong> ：消息队列减少了服务之间的耦合性，不同的服务可以通过消息队列进行通信，而不用关心彼此的实现细节。</li><li><strong>异步处理</strong> ：消息队列本身是异步的，它允许接收者在消息发送很长时间后再取回消息。</li><li><strong>流量削锋</strong> ：当上下游系统处理能力存在差距的时候，利用消息队列做一个通用的”载体”，在下游有能力处理的时候，再进行分发与处理。</li><li><strong>日志处理</strong> ：日志处理是指将消息队列用在日志处理中，比如 Kafka 的应用，解决大量日志传输的问题。</li><li><strong>消息通讯</strong> ：消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯，比如实现点对点消息队列，或者聊天室等。</li><li><strong>消息广播</strong> ：如果没有消息队列，每当一个新的业务方接入，我们都要接入一次新接口。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，是下游的事情，无疑极大地减少了开发和联调的工作量。</li></ul><h2 id=rocketmq>RocketMQ<a hidden class=anchor aria-hidden=true href=#rocketmq>#</a></h2><figure><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240915162312625.png alt=image-20240915162312625></figure><p>RocketMQ的相关组件和角色：</p><ul><li>producer：生产者，消息的发送者。负责将消息发送给Broker</li><li>producer group：生产者组，标记一类生产者</li><li>nameSrv：（看板）名字服务，一个路由注册中心。Broker会定时把自己的信息（IP等）上传给nameSrv，生产者和消费者就可以从nameSrv获取这些信息，才能顺利发送和接收消息</li><li>Broker：代理服务器，消息中转站，负责消息的存储、投递、查询</li><li>Broker cluster：代理服务器集群。可以是主主集群（集群内部Broker同级别同时对外提供服务），主从集群（主B对外提供服务，从B同步主B的消息作为备份防止主B宕机，不对外提供服务）。一个主可以有多个从</li></ul><ul><li>consumer：消费者，向Broker拉取自己想要消费的消息</li><li>consumer group：两个消费者组之间消费的消息互不干扰（比如一条消息可以被A组看也可以被B组看，想看多少各自随便）</li></ul><p>对一个consumer group来说，消费的模式有：集群消费和广播消费</p><ul><li><p>集群模式：大家<strong>负载均衡</strong>地消费了这4条消息</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240915203359941.png alt=image-20240915203359941></p></li><li><p>广播模式：组内每个消费组都收到了<strong>全部</strong>消息</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240915203811232.png alt=image-20240915203811232></p></li><li><p>Topic：主题，消息的分类。这样对某一类消息感兴趣的消费者则可以订阅Topic-xxx，不会被无关的消息给冲烂了。</p></li><li><p>Tag：二级分类，更精细化区分消息</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240915204539649.png alt=image-20240915204539649></p></li></ul><blockquote><p>思考：不同的消费组之间可以全量消费同Topic的消息，它们之间的消费进度又可以不一样，是Broker将信息复制了多份吗？</p></blockquote><h3 id=mq两种基本实现模式>mq两种基本实现模式<a hidden class=anchor aria-hidden=true href=#mq两种基本实现模式>#</a></h3><h4 id=队列模式>队列模式<a hidden class=anchor aria-hidden=true href=#队列模式>#</a></h4><p>生产者发送的消息被排成队列，然后消费者们<strong>竞争</strong>消费队列上的消息。按照队列的特性，消息被消费了等于出队，即从队列上被移除了，那么每条消息只会被一个消费者消费，因此消费者之间是竞争关系。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240918213901708.png alt=image-20240918213901708></p><p>缺点：一个消息可能有很多消费者都感兴趣，但是他们之间又不是竞争消费的关系，即这些消费者都想消费所有的消息。这时候普通的队列模式就不合适了。-> 复制到多队列，加重存储负担，冗余</p><h4 id=发布-订阅模式>发布-订阅模式<a hidden class=anchor aria-hidden=true href=#发布-订阅模式>#</a></h4><p>生产者发布消息，消费者订阅消息。订阅依据就是Topic主题。</p><p>想要实现的功能：比如我往 Topic-LOL 这个主题发布消息，那么订阅了这个主题的消费者都能收到这个消息，我往 Topic-DOTA 这个主题发布消息，那么订阅了 DOTA 主题的消费者都能收到 DOTA 相关的消息。</p><p>具体实现：引入<strong>消息位置（offset）</strong>，类比数组下标。我们的述求是消息可以被多个消费者消费，那么<strong>只需维护每个消费者已经消费到的位置，每当消费者消费一条消息，消费位置就+1</strong>，然后消费者根据记录的消息位置去消费对应的数据即可。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240918214346401.png alt=image-20240918214346401></p><blockquote><p>问题：同一个消费组内的消费者如何消费消息？让他们竞争同一个消费位置吗？那岂不是需要等上一个消费者消费完了，组内其他消费者才能消费下一条消息？这效率就很低了。</p><p>-> 引入：<strong>在 RocketMQ 中叫队列</strong>（这个跟数据结构上的队列在概念上要区分下），<strong>在Kafka中叫分区</strong>。</p></blockquote><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240918215204063.png alt=image-20240918215204063></p><p>可以看到，发往一个 Topic 的消息，实际上不是在一个队列里，而是分布在多个队列中。这样属于一个消费组的消费者们可以专门负责主题里面的一个队列。</p><p>消费点位的<strong>记录维度就变成了 Topic-消费组-队列</strong>，比如现在一共有两个主题，分别是 Topic-LOL、Topic-DOTA，每个主题都有两个队列（分区）。</p><p>如果c组有加了一人c，对应Topic也可以加一个队列；另一个c组人数不变，可以让其中一人多消费一个队列即可。</p><blockquote><p>处理消息堆积：加队列同时加消费者（面试常问）</p></blockquote><p>至此，我们就清晰了<strong>企业级消息队列实现的发布-订阅模式的核心原理</strong>：即 Topic 下分队列（分区），然后维护每个消费组在每个 Topic 下每个队列的消息位置，以消息位置（offset）来控制消息消费的进度。</p><blockquote><p>问题：一个Topic里有多个队列，生产者如何确定往哪个队列发消息？</p><ul><li>轮询：比如生产者-A，要往 Topic-LOL 发送消息，那么第一条发给队列-1，第二条发给队列-2，第三条发给队列-3，第四条发给队列-1，第五条发给队列-2。。。。如此往复即可。这样每个队列的消息量平均，对应消费者的工作量也平均。</li><li>指定发往某个队列：比如有关匹配的消息都发往队列-1，有关大乱斗的消息都发往队列-2，有关云顶之弈的消息都发给队列-3。</li></ul></blockquote><p>RabbitMQ 的底层就是队列模式，而 RocketMQ 和 Kafka 两者的实现都是发布-订阅模式。</p><h3 id=broker消息存储>Broker消息存储<a hidden class=anchor aria-hidden=true href=#broker消息存储>#</a></h3><blockquote><p>消息存储非常重要，mq一个关键功能就是<strong>削峰填谷</strong>。比如在大流量场景下（双十一），一下子涌入大量用户，造成请求高峰，使系统压力剧增。</p><p>解决：把相关请求先发送到mq，然后直接返回success给用户。如：“业务受理成功，由于当前访问较多，详情约在xx-xx分钟后可查询“这样的话术。然后让系统平缓、匀速地拿取mq里的消息再进行业务处理。</p><p>而这个过程的重点就是<strong>保证排队消息存储的可靠性</strong>。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240918231411306.png alt=image-20240918231411306></p><p>so，，Broker里的消息应该存到哪里呢？</p></blockquote><ul><li>存数据库（MySQL）？zszz，性能不行；</li><li>那缓存Redis？性能是ok，but不靠谱；</li></ul><p>对于一个中间件，mq本身轻量级也很关键，中间件依赖别的组件，就是强行使用者维护更多的组件，会变得更复杂。</p><ul><li>所以存<strong>本地硬盘</strong>。其实mysql保存和redis持久化的数据也都是在本地硬盘，所以直接跳过中介，把消息直接写入本地硬盘的文件上。现在硬盘都是RAID，即多块硬盘组成逻辑上的一块，也就是有备份。</li></ul><p>那么mq中的消息要以什么格式存储到文件里呢？</p><ul><li><p><strong>commitlog</strong> ：类似记事本一样，一行一行的写</p></li><li><p>每条消息的大小不固定；</p></li><li><p>不同topic的消息都存在一个文件里（why都写进一个文件？涉及计组的底层机械硬盘结构）</p></li></ul><blockquote><p>计组知识点</p><figure><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920144525135.png alt=image-20240920144525135></figure><ul><li>寻道时间</li><li>旋转时间</li></ul><p>在物理上，<strong>如果存储在硬盘上的数据在同一个磁道且相邻的扇区，那么根据硬盘的机械运行轨迹，读/写的顺序就非常快</strong>（省略寻道时间，顺应旋转的方向），甚至可以媲美内存的读写速度！<strong>顺序读 顺序写</strong></p></blockquote><blockquote><p><strong>顺序写就是RocketMQ为什么性能好的原因（面试问）其他应用：mysql的redo log</strong></p></blockquote><p>so，所有的消息不论是哪个 topic 都写入同一个文件，并且都是顺序追加写入，那么对应到硬盘上就是顺序写！（基本上一个文件的内容，如果空间足够，那么都是连续的）这样就大大提高了消息队列写消息的性能。</p><p><strong>如果将不同的 topic 存入不同的文件，我们无法保证这些文件在物理上的位置是连续的。</strong>(随机写)</p><blockquote><p>总结：
RocketMQ 选择将消息写入到文件中，依赖机械硬盘（当然也可以是ssd）来保证消息存储的可靠性，并且根据机械硬盘的特性，把不同 topic 的消息都追加写入到一个叫 commitlog 的文件中，这样的顺序写性能很好。</p></blockquote><p>现在我们知道消息都存储在一个叫commitlog的文件里，那么队列呢？队列和commitlog是什么关系？消费者如何快速获取commitlog里的消息？</p><h3 id=consumequeue>ConsumeQueue<a hidden class=anchor aria-hidden=true href=#consumequeue>#</a></h3><p>当消息被写入commitlog时表示生产者成功发送了消息，消息也落入硬盘被持久化了。此时可以启动一个定时任务 (约1ms/次，几乎实时)，将新写入commitlog的消息转发给consumeQueue(就是Topic队列)</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920145521661.png alt=image-20240920145521661></p><blockquote><p>consumequeue有必要存全量的消息吗？完全copy一份commitlog也太消耗内存了。</p><p>所以直接去cl里找。由于消息直接存入cl，且是追加写入，里面存储了连续的消息。引入<strong>偏移量作索引</strong>。</p></blockquote><p>consumerQueue 实际存储的不是消息本身，而是存储消息在 commitlog 文件中的<strong>起始偏移量和本条消息的长度</strong>还有tagHash。</p><figure><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920145920276.png alt=image-20240920145920276></figure><p>时间换空间 也需要落盘存储</p><p>consumerOffset 和 commitlog offset：</p><ul><li><p>consumeroffset <strong>消息（逻辑）位置</strong>：在一个队列被多个消费者消费时，只需维护每个消费者已经消费到的位置。每当消费一条消息，offset（消费位置）+1，然后消费者根据记录的offset去队列里查找消费对应的消息。</p></li><li><p>commitlog里的offset：消息所在commitlog文件中的偏移位置。通过这个offset找到消息的起始位置再+size获取完整消息</p></li></ul><p>至此，<strong>消费者消费一条消息的流程：</strong></p><p>通过consumerOffset找到consumerQueue里的内容，这个内容存储了消息所在commitlog中的offset和size，然后通过这两个数据去commitlog里获取完整的消息内容。</p><h4 id=消息索引-indexfile>消息索引 IndexFile<a hidden class=anchor aria-hidden=true href=#消息索引-indexfile>#</a></h4><blockquote><p>通过输入索引快速定位到一条消息。</p></blockquote><ul><li><p>HashMap，但文件不提供HashMap的功能</p></li><li><p>IndexFile结构：</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920150927877.png alt=image-20240920150927877></p><ul><li><p>head：大小固定，存放一些元信息（当前文件中消息的最小和最大存储时间、当前文件中消息在commitlog的最小和最大偏移量、已用hash槽个数、已用index个数）</p></li><li><p>每个槽大小固定4字节</p></li></ul><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920151054938.png alt=image-20240920151054938></p><p>知道是第几个槽后，槽里存的是index item下标。每个index item大小固定20字节。一个indexFile文件有2000w个ii，存储了消息再commitlog中的物理offset。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920151352555.png alt=image-20240920151352555></p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920151422224.png alt=image-20240920151422224></p><ul><li><p>假如发生了hash冲突怎么办？</p><p>更新槽的值为最新的index item下标，1。并且记录上一个index item 的下标，prevIndex = 0。同时index item里还要存储keyhash来判断跟当前计算的hash是否一致，不一致说明冲突，需要通过prevIndex值去得到上一个item的下标再去查找。<img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920151532293.png alt=image-20240920151532293></p><p>index item里还存了消息的时间，这个时间还是个差值：这条消息在commitlog写入的时间（IndexFile第一条消息的写入时间）</p></li></ul></li></ul><h3 id=消息发送>消息发送<a hidden class=anchor aria-hidden=true href=#消息发送>#</a></h3><p>消息类型：</p><ul><li><p>普通消息</p></li><li><p>同步消息：producer发送一条消息给Broker，需要等待Broker返回响应才会继续发送后续消息</p><p><code>producer.send(msg)</code></p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920153136674.png alt=image-20240920153136674></p><p>要是没收到reaction，生产者会进行重试。默认重试3次，3次后还是失败就抛异常。此时需要我们捕获这个异常，进行日志记录或其他兜底操作。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920153407507.png alt=image-20240920153407507></p><p>（类似TCP的单词握手，一次request过去，一个ack回来）</p><p>但酱紫重试的缺点是<strong>消息重复发送</strong></p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920153540083.png alt=image-20240920153540083></p><p><strong>这里要做好消息的防重，或幂等（？）</strong></p><blockquote><p>问：如何保证消息一定发送成功？</p><p>答：Broker接收成功后，返回ack给生产者，没接收到ack则不认为消息发送成功，进行重试。</p></blockquote></li><li><p>异步消息</p><p>不需要阻塞等待上一条的ack可以紧接着发送后续消息</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920154143390.png alt=image-20240920154143390></p><p>假如前面的消息发送失败了，会有另外的线程来处理Broker的响应。成功响应就执行onSuccess逻辑，失败则执行onException</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=n>producer</span><span class=p>.</span><span class=na>send</span><span class=p>(</span><span class=n>msg</span><span class=p>,</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>SendCallback</span><span class=p>(){</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> </span><span class=kd>public</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>onSuccess</span><span class=p>(</span><span class=n>SendResult</span><span class=w> </span><span class=n>sendResult</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>   </span><span class=c1>//do sth</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> </span><span class=kd>public</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=nf>onException</span><span class=p>(</span><span class=n>Throwable</span><span class=w> </span><span class=n>e</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>   </span><span class=c1>//do sth</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>   </span><span class=c1>// 失败信息记录</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w> </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>})</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>异步消息也能设置重试，问题和同步的一样（重复）。适合对响应时间敏感度的场景。</p></li><li><p>单向消息</p><p>生产者只管发消息，不关心Broker收没收到，也不用等ack</p><blockquote><p>适合对消息可靠性要求不高的场景: 大量日志收集（不在乎丢几条）</p></blockquote></li><li><p>顺序消息</p><p>按顺序发布消息，按发布顺序消费消息</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920154858349-1726818541342-30.png alt=image-20240920154858349></p><blockquote><p>场景：1. 创建订单 2. 支付 3. 发货 4. 完结订单</p></blockquote><p>由于一个Topic多队列的实现，普通消息没法保证先发送的一定先被消费。</p><blockquote><p>解决方案：把这几个消息都发一个队列里！把这几类消息都发进一个队列，<strong>这种叫全局顺序消息</strong></p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920155234044.png alt=image-20240920155234044></p><p>如果把<strong>同一笔</strong>订单的创建支付发货收货发往一个队列，<strong>不同订单发往不同队列</strong>，叫<strong>分区顺序消息</strong>. 能提高并发度和消费速度。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920155533584.png alt=image-20240920155533584></p><p>分区依赖一个sharding key，比如订单场景中的订单号可以作为sharding key。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>queueIndex</span> <span class=o>:=</span> <span class=nx>orderId</span> <span class=o>%</span> <span class=nb>len</span><span class=p>(</span><span class=nx>queue</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nx>producer</span><span class=p>.</span><span class=nf>send</span><span class=p>(</span><span class=nx>msg</span><span class=p>,</span> <span class=nx>queueIndex</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>默认普通消息是轮询选择队列。（上一个发队列1，下一个发队列2，。。。）</p></blockquote></li><li><p>延迟消息</p><p>生产者发送了消息，希望延迟一段时间再被消费。</p><blockquote><p>实现：</p><p>延迟消息一开始不放在正常的Topic中，有一个专门的Topic叫SCHEDULE_TOPIC_XXXX，放置所有的延迟消息。等到时间了再把该消息发回本身的Topic队列。这样就不会妨碍消费其他消息。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920160418731.png alt=image-20240920160418731></p><p>RocketMQ中的延迟时间只能设定投递等级，不同等级固定对应一个时间。</p></blockquote></li><li><p>批量消息</p><p>一次发包发送多条消息，适用对<strong>吞度量敏感的场景</strong>。</p><p>100条打包成1条发，只调用一次接口，只等待一次reaction。吞度效率变高了，但一条消息有错就要一批重来。</p></li><li><p>事务消息</p></li></ul><h3 id=namesrv-命名服务>NameSrv 命名服务<a hidden class=anchor aria-hidden=true href=#namesrv-命名服务>#</a></h3><h4 id=namesrv和broker>NameSrv和Broker<a hidden class=anchor aria-hidden=true href=#namesrv和broker>#</a></h4><p>在 Broker 启动之前，NameSrv 需要先启动。<strong>Broker 会定时的把自己关于 Topic 的信息上报至 NameSrv，NameSrv 会维护记录这些信息，并且将无用的 Broker 剔除</strong>。</p><figure><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920161234206.png alt=image-20240920161234206></figure><h4 id=namesrv和producer>NameSrv和Producer<a hidden class=anchor aria-hidden=true href=#namesrv和producer>#</a></h4><p>对于 Producer 来说，启动后它需要找到一台 NameSrv 建立长连接。Producer 需要从 NameSrv 或者有关 Topic 的路由信息，<strong>然后跟对应的 Broker 建立长连接，后面直接将消息发送给对应的 Topic</strong>。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920161631007.png alt=image-20240920161631007></p><blockquote><p>注意：Producer 不会缓存路由信息到本地文件中，只会放在内存里。所以如果 NameSrv 挂了，由于 Produer 是直接跟 Broker 建立长连接直接发送消息的，所以不影响现有的消息发送。</p><p>但是如果这时候你将 Producer 重启了，那么在 NameSrv 挂了的情况下，消息就无法正常发送了，因为路由信息都丢了，也无法从 NameSrv 获取。不要随意重启Producer</p></blockquote><h4 id=namesrv和consumer>NameSrv和Consumer<a hidden class=anchor aria-hidden=true href=#namesrv和consumer>#</a></h4><p>Consumer和Producer一样，启动后先和NameSrv建立长连接，获取topic和broker的映射。然后和有关broker建立长连接。发消息和消费消息的过程都不会经过中间商NameSrv，NameSrv 的作用就是“看板”，来记录维护路由信息，给 Consumer 和 Producer 指路。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920162545518.png alt=image-20240920162545518></p><h4 id=namesrv和namesrv>NameSrv和NameSrv<a hidden class=anchor aria-hidden=true href=#namesrv和namesrv>#</a></h4><p>为了避免单点NameSrv挂了导致新P和C成瞎子，在生产中需要部署多台NameSrv形成集群。<strong>RocketMQ 实现的 NameSrv 非常轻量级，集群内的 NameSrv 是互相独立的存在，它们之间不会进行任何的信息交互</strong>。</p><ul><li>Broker需要给集群内的每一台NameSrv都上报路由信息。</li><li>对 Producer 和 Consumer 来说，因为多台 NameSrv 的数据都是一样的，因此它们只需要随机选择集群内的一台 NameSrv 进行长连接即可获取全量的路由信息。</li></ul><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920163829468.png alt=image-20240920163829468></p><p>总结：</p><p>NameSrv 这个角色就是动态路由中心，会维护存活的 Broker 信息，记录 Topic 的路由关系，使得 Producer 和 Consumer 可以正确的找到对应的 Broker 。</p><p>RocketMQ 简单的自己设计了一个轻量级路由中心而不是依赖于其他三方组件，这样对整个消息队列中间件而言更轻量和自主。</p><p><strong>kafka中对应NameSrv的角色是zookeeper</strong>。zk 集群中多台机器会进行数据同步，就会产生数据一致性问题，NameSrv 则直接每台都存储全量的数据，且之间不进行任何信息交互，完美地避开数据不一致的问题。</p><p>kafka 在新版本中已经移除了 zk 的强依赖（强一致性在性能方面开销会比较大）。</p><blockquote><p>新问题：<strong>消息存储在Broker，是消费者去Broker拉取消息pull还是B主动给消费者推push呢？</strong></p><p>答：看需求</p></blockquote><p>消息vs快递 ：上门送货 = push， 自取 = pull</p><h4 id=推消息>推消息<a hidden class=anchor aria-hidden=true href=#推消息>#</a></h4><p>Broker收到对应消息，立马推送给消费者</p><p>优点：实时性超高，对消费者实现也简单；</p><p>缺点：消费者在某个时段超忙，没空处理新消息；Broker疯狂push，可能直接把消费者干爆；这就违背了消息队列<strong>削峰填谷</strong>的初衷。所以Broker在push消息前可以先打探消费者的状态再推，但消费者太多的情况也不合适。</p><p>使用场景：消费者不多、消息量不大、及时性要求高</p><h4 id=拉消息>拉消息<a hidden class=anchor aria-hidden=true href=#拉消息>#</a></h4><p>优点：Broker轻松，只需要存储好生产者发来的消息，等消费者来拉就好了，想拉多少拉多少，不用维护其他关系；</p><figure><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920165504049.png alt=image-20240920165504049></figure><p>缺点：消息的消费可能不及时</p><figure><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920165651686.png alt=image-20240920165651686></figure><p>如果把定时时间调短一点，消费不及时可以改善，但又造成了<strong>消息忙请求</strong>。就是说几个小时都没消息过来，消费者还隔1s来要一次。</p><p>目前RocketMQ和kafka用的都是<strong>拉模式 pull，但是长轮询</strong>：</p><blockquote><p>消费者发送拉取请求到Broker时，如果此时有消息，则Broker直接响应返回消息；如果没消息就hold这个请求，比如等待15s，15s内要是有消息过来了就立马响应返回。</p><p>这样既避免了忙请求，也提升了消息的及时性ovo</p></blockquote><p>RMQ有pullConsumer和pushConsumer，但底层实现都是基于长连接的长轮询去拉取消息。</p><h4 id=消费者启动>消费者启动<a hidden class=anchor aria-hidden=true href=#消费者启动>#</a></h4><p>怎么获取nameSrv？在开始时注入nameSrv地址</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920173224201.png alt=image-20240920173224201></p><p>接下来<strong>分赃</strong>：</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920173502624.png alt=image-20240920173502624></p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920173516883.png alt=image-20240920173516883></p><p>新来一个消费者分俩给它，减轻老消费者的压力同时提高消费效率。</p><p>分赃的契机：新消费者启动并连上Broker</p><p>分赃过程：由消费者遵循一定规则主动分配 （专名：==重平衡==，也是==客户端的负载均衡==）</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920174437361.png alt=image-20240920174437361></p><blockquote><p>消费者5看似没活干了，会这么摸鱼吗？</p><p>问：<strong>消息堆积时，增加消费者有用吗？</strong></p><p>答：我不好说。如果这个消费者组的消费者数量比Topic的队列数量小，此时可以加消费者来缓解消息堆积；如果消费者数量比队列还多，就没用力。</p><ul><li>不光在消费者启动时会触发重平衡，每个消费者会有个定时任务，每20s重平衡一下子，就负载均衡了，防止消费任务不均衡。</li><li>如果有消费者下线了，由于和Broker有连接，下线了Broker也知道，会通知所有消费者重平衡一下。</li></ul><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920175212867.png alt=image-20240920175212867></p><p>实现了<strong>动态负载均衡</strong>：业务高峰时增加消费者，业务低谷时减少消费者。</p></blockquote><h4 id=怎么拉消息>怎么拉消息<a hidden class=anchor aria-hidden=true href=#怎么拉消息>#</a></h4><p>最简单的想法是一个队列对应一个线程负责去pull，但上下文切换过于频繁。线程池？也用不上</p><p>RocketMQ只用了一个线程（PullMessageService）来执行pull操作，所有的pull动作都会封装成pullRequest，扔到pullRequestQueue一个阻塞队列中。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920175825924.png alt=image-20240920175825924></p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920175905748.png alt=image-20240920175905748></p><p>pullRequest获取的结果会先缓存到ProcessQueue中，同时再构建一个消费任务consumerRequest 给ConsumeMessageService这个线程池来消费消息。</p><p><img loading=lazy src=/%e5%85%ab%e8%82%a11.assets/image-20240920180310432.png alt=image-20240920180310432></p><p><strong>ProcessQueue: 起到暂缓消息的作用，用来流控</strong>。如果暂缓的消息很多，说明消息处理慢，消息在消费者这里有堆积，这时候就需要限制拉取速度。也就是不能一直构建pullRequest，要等一等（流控）</p><ul><li><p>消息点位 offset：之前提到的每个消费组需要维护Topic下的每个队列被消费到的点位，这个点位是怎么保存的呢？在什么时候提交呢？</p><p>答：在不同模式下，保存方式不同</p><blockquote><p>广播模式：</p><p>​ 广播模式下存储在消费者<strong>本地磁盘</strong>上，因为消息广播给每个消费者，不需要统一一个地方来管，每个消费者自己维护就行了。</p><p>集群模式：</p><p>​ 不能本地维护了，因为同个消费组内的消费者是互帮互助的关系。在新消费者顶替旧消费者时需要知晓前一个人的消费进度。所以集群模式下offset<strong>存储在Broker里</strong>，这样顶上的消费者可以从Broker获取offset。</p><blockquote><p>那么，<strong>消费者具体在什么时候将自己的消费位置告知Broker更新呢？</strong></p><p><strong>答：拉取信息的时候</strong>顺带把消费点位提交给Broker。</p><p>问题又来了：假设消费完了，还没来得及拉消息，消费者就挂了，此时存在Broker里的点位不就不准了吗？</p><p>答：确实，按照这个设计此时如果发生了重平衡，就会导致消息重复消费。这个设计没法保证消息只会被消费一次。因此我们只能做好<strong>消费者的幂等</strong>，<strong>使得消费多次消息和消费一次达到的效果一样</strong>。</p><p><strong>扩展：常用的业务幂等性保证方法</strong></p><p><a href=https://cloud.tencent.com/developer/article/2371740>一文讲透消息队列RocketMQ实现消费幂等-腾讯云开发者社区-腾讯云 (tencent.com)</a></p><ul><li>只靠mq无法实现业务幂等性，要结合业务场景</li><li><strong>利用数据库的唯一约束</strong>实现幂等：如将订单表中的订单编号设置为唯一索引，创建订单时，根据订单编号就可以保证幂等；</li><li><strong>去重表</strong>：和上一个本质相同，首先在去重表上建唯一索引，其次操作时把业务表和去重表放在同个本地事务中，如果出现重现重复消费，数据库会抛唯一约束异常，操作就会回滚</li><li><strong>利用redis的原子性</strong>：每次操作都直接set到redis里，然后将redis数据定时同步到数据库中；</li><li><strong>多版本（乐观锁）控制</strong>：适用于更新场景。给业务数据增加一个版本号属性，每次更新数据前，比较当前数据的版本号是否和消息中的版本一致，如果不一致则拒绝更新数据，更新数据的同时将版本号+1</li><li><strong>Token机制</strong>：生产者发送每条数据的时候，增加一个全局唯一的id，这个id通常是<strong>业务的唯一标识</strong>，比如订单编号。在消费端消费时，则验证该id是否被消费过，如果还没消费过，则进行业务处理。处理结束后，在把该id存入redis，同时设置状态为已消费。如果已经消费过了，则不进行处理。</li><li>状态机机制：多用于更新且业务场景存在多种状态流转的场景</li></ul></blockquote></blockquote></li></ul><h4 id=怎么消费消息>怎么消费消息<a hidden class=anchor aria-hidden=true href=#怎么消费消息>#</a></h4><p>前面提到 pull 到消息后会构建ConsumeRequest 提交到线程池 ConsumeMessageService。</p><p>线程池有两个实现：</p><ul><li>并发消费（常用）</li><li>顺序消费</li></ul><p>假设线程池消息消费失败了咋办？</p><ul><li><p>RMQ的实现是将消息消费失败的消息返回发送给Broker，这个消息会被发送给一个特定的<strong>重试Topic</strong>： “%RETRY%+consumerGroup”</p><p>这样就失败消息就不会阻塞原来Topic上的其他消息，而是在重试队列上等待后续的重试消费；如果多次重试还是失败（默认16次），就会把它打入死信队列，需要人工介入处理。</p></li></ul><h3 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h3><p>消息在mq上生产、消费的关键链路流程:</p><ul><li>nameSrv启动，待命；</li><li>Broker启动，把自己的IP、端口、Topic等上传给nameSrv，等待Producer发送消息，等待Consumer消费消息；</li><li>Producer启动，连接nameSrv并获取Broker信息，跟对应Broker建立连接，连上之后发消息给Broker；</li><li>Broker将消息存储到commitlog文件中，并分发到consumeQueue，等待consumer来拉取消息消费；</li><li>Consumer启动，连接nameSrv并获取Broker信息，跟对应Broker建立连接，连上后发送pullRequest给Broker；</li><li>Broker根据对应的Topic、队列ID和消息点位，找到consumeQueue中的消息，再解析找到对应commitlog得到消息内容，返回给Consumer;</li><li>Consumer消费消息，随后上报自己的消费进度给Broker。</li></ul><p>【持续更新&mldr;】</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://erica423.github.io/tags/%E5%B7%A5%E4%BD%9C/>工作</a></li></ul><nav class=paginav><a class=prev href=https://erica423.github.io/posts/first/><span class=title>« Prev</span><br><span>【完】使用Github Actions实现Hugo静态博客自动部署</span>
</a><a class=next href=https://erica423.github.io/posts/algorithm/><span class=title>Next »</span><br><span>算法</span></a></nav></footer><div id=waline></div><script>Waline.init({el:"#waline",dark:"body.dark",serverURL:"https://waline.vercel.app"})</script></article></main><footer class=footer><span>&copy; 2024 <a href=https://erica423.github.io/>Erica-Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>本站总访问量<span id=busuanzi_value_site_pv></span>次
</span><span id=busuanzi_container_site_uv>本站访客数<span id=busuanzi_value_site_uv></span>人次</span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>